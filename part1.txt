Azure data Factory (ADF) - ingestion
Azure Databricks - for data processing
Azure SQL DB - EMR tables
Azure SA - raw files, parquet files
Key Vault - for storing credentials





Azure SA (ADLS Gen2)- ttadlsdev
	- Landing
	- Bronze
	- Silver
	- Gold
	- Config (Metadata driven pipeline)
		○ Emr
			§ load_congif.csv
			database,datasource,tablename,loadtype,watermark,is_active,targetpath
trendytech-hospital-a,hos-a,dbo.encounters,Incremental,ModifiedDate,0,hosa
trendytech-hospital-a,hos-a,dbo.patients,Incremental,ModifiedDate,0,hosa
trendytech-hospital-a,hos-a,dbo.transactions,Incremental,ModifiedDate,0,hosa
trendytech-hospital-a,hos-a,dbo.providers,Full,,0,hosa
trendytech-hospital-a,hos-a,dbo.departments,Full,,0,hosa
trendytech-hospital-b,hos-b,dbo.encounters,Incremental,ModifiedDate,0,hosb
trendytech-hospital-b,hos-b,dbo.patients,Incremental,Updated_Date,1,hosb
trendytech-hospital-b,hos-b,dbo.transactions,Incremental,ModifiedDate,0,hosb
trendytech-hospital-b,hos-b,dbo.providers,Full,,0,hosb
trendytech-hospital-b,hos-b,dbo.departments,Full,,0,hosb
	
	
	
	
	
ADF PIPELINE
	EMR(Azure SQL DB) -> ADLS Gen2 (Bronze folder in parquet format)
	
	We will create audit table( Delta table )
	
	- Linked Service
		○ Azure SQL DB
		○ ADLS Gen2
		○ Delta Lake
		○ Key Vault (Storing credentials and passwords)

	- Dataset
		○ Azure SQl DB  - Db name (linked service) / table name / schema name
		○ Config file - Delimited Text (ADLS Gen2)  - configs/emr/load_config.csv
		○ Parquet (ADLS Gen2)
		○ Databricks delta Lake - schema name / table name

Pipeline ->
	Dataset ->
		Linked Service
	
	- Activities
		○ LookUp - will read config file
		○ For Each -
			§ Get Metadata -  if parquet file exists in bronze layer
			§ True  - copy data (move to archive)  
				□ File path - targetpath / archive/ year/ month /day
			§ If loadtype = full
				□ Copy to bronze
				□ Lookup - insert logs full load
					® @concat('insert into audit.load_logs(data_source,tablename,numberofrowscopied,watermarkcolumnname,loaddate) values (''',item().datasource,''', ''',item().tablename,''',''',activity('Full_Load_CP').output.rowscopied,''',''',item().watermark,''',''',utcNow(),''')')
			§ If condition false
				□ Lookup - check the audit table (Fetch logs) - get max date to compare 
					® @concat('select coalesce(cast(max(loaddate) as date),''','1900-01-01',''') as last_fetched_date from audit.load_logs where',' data_source=''',item().datasource,''' and tablename=''',item().tablename,'''')
				□ CopyData 
					® @concat('select *,''',item().datasource,''' as datasource from ',item().tablename,' where ',item().watermark,' >= ''',activity('Fetch_logs').output.firstRow.last_fetched_date,'''')
				□ Insert logs inc load
					@concat('insert into audit.load_logs(data_source,tablename,numberofrowscopied,watermarkcolumnname,loaddate) values (''',item().datasource,''', ''',item().tablename,''',''',activity('Incremental_Load_CP').output.rowscopied,''',''',item().watermark,''',''',utcNow(),''')')
					
	- Pipeline

It is squential run - because auto increment identity column
